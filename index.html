<!doctype html><meta charset=utf-8>

<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-152598381-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-152598381-4');
</script>

<meta property="twitter:image" content="https://versechow.github.io/monodepth-vio-init/thumbs/tr_paper-1.jpg" />
<meta property="og:image" content="https://versechow.github.io/monodepth-vio-init/thumbs/tr_paper-1.jpg"/>
<meta property="og:type" content="website"/> 
<meta property="og:url" content="https://versechow.github.io/monodepth-vio-init/" /> 
<meta property="og:title" content="Learned Monocular Depth Priors in Visual-Inertial Initialization" />
<!-- <meta property="og:description" content="With light weight Monocular depth model, VIO can be initialized faster and more robust." />
 --></head>

<title>Learned Monocular Depth Priors in Visual-Inertial Initialization</title>

<style>
    body {
      font-family: Roboto, "Roboto", sans-serif;
      background: #F0F0F0;
      color: #696969;
      /* text-align: center; */
      /* padding: 0px 40px 40px 40px; */
    }
    h1 { 
        display: block;
        font-size: 2em;
        margin-top: 0.67em;
        margin-bottom: 0px;
        margin-left: 0;
        margin-right: 0;
        font-weight: normal;
        color: #000;
        }
    h2 { 
        display: block;
        font-size: 1.5em;
        margin-top: 0.67em;
        margin-bottom: 0.67em;
        margin-left: 0;
        margin-right: 0;
        font-weight: normal;
        color: #000;
        }
    h3 { 
        display: block;
        font-size: 1em;
        margin-top: 0.67em;
        margin-bottom: 0.67em;
        margin-left: 0;
        margin-right: 0;
        font-weight: normal;
        }
    h4 { 
        display: block;
        font-size: 1.1em;
        margin-top: 0.0em;
        margin-bottom: 0.67em;
        margin-left: 0;
        margin-right: 0;
        font-weight: normal;
        color: #000;
        }
    hr {
        border-top: 1px solid black;
    }
    .first-letter {
        font-weight: 350;
        display: inline;
        font-size: 1.75em;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0px;
        margin-inline-end: 0px;
    }
    .paper-thumbnail {
      box-shadow: 1px 2px 5px 2px rgba(0, 0, 0, .15);
      width: 230px;
    }
    .grid-container {
        display: grid;
        grid-template-columns: auto auto auto auto;
        text-align: center;
        margin: 0px -10px 0px -10px;
    }
    .grid-container > div {
        text-align: center;
    }
    .logos {
      margin: 0px 10px 10px 0;
    }
    .ib {
      display: inline-block;
      margin: 00px 20px 20px 0px;
    }
    .logos-container {
	margin: 20px 0px 50px 0px;
    width: 100%;
    float: left;
	position: relative;
    min-width: 800px;
    }
    .google-logo {
    text-align: center;
	position: relative;
    }

    .video {
	margin: 20px 0px 0px 0px;
    width: 100%;
    float: left;
	position: relative;
    }
    .abstract {
	margin: 50px 0px 0px 0px;
    width: 100%;
    float: left;
	position: relative;
    }
    .data {
	margin: 40px 0px 80px 0px;
    width: 100%;
    float: left;
	position: relative;
    }
    .superhero {
    width: 1024px;
    position: relative;
    margin: auto;
    padding-left: 80px;
    padding-right: 80px;
    overflow: auto;
    /* min-width: 800px; */
    }
    .title {
	margin: 50px 0px 0px 0px;
    width: 100%;
    float: left;
	position: relative;
    text-align: center;
    /* line-height: 40pt; */
    }
    .title-line {
        margin: 20px 0px 20px 0px;
    }
    .authors {
	/* margin: 25px 0px 0px 0px; */
    /* width: 400px;
    height: 1100px; */
    padding-left: 15%;
    padding-right: 15%;
    float: left;
    position: relative;
    text-align: center;
    }
    .paper {
    width: 100%;
    float: left;
	position: relative;
    }
    h1.title-name {
	font-size: 40px;
	font-weight: 300;
    margin-bottom: 0px;
	display: inline-block;
    }
    h1.name {
	font-size: 40px;
	font-weight: 300;
    margin-bottom: 10px;
	display: inline-block;
    }
    .email {
	display: block;
	font-size: 19px;
	font-weight: 300;
	color: #81899C;
	margin-top: 0px;
    display: inline-block;
    }

    .lead {
	font-size: 20px;
	font-weight: 300;
	margin-top: 0px;
	line-height: 30px;
    display: inline-block;
    }
    .names {
    font-size: 1.05em;
	font-weight: 300;
	margin-top: 0px;
	line-height: 20pt;
    display: inline-block;
    text-align: center;
    }
    .video-container {
    position: relative;
    padding-bottom: 56.25%; /* 16:9 */
    height: 0;
    }
    .video-container iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    }
    .footer {
        width: 100%;
        height: 65px;
        float: left;
        margin-top: 25px;
    }
    .footer-googleai {
        width: 20%;
        float: left;
    }
    .footer-google {
        width: 20%;
        float: right;
        text-align: right;
    }
    a.pagelink:link,
    a.pagelink:visited,
    a.pagelink:active { color: #81899C; text-decoration: none; }
    a.pagelink:hover { color: #006699; text-decoration: underline; }
</style>

<body>
<div class="superhero">
    <div class="title">
        <h1 class="title-name">
        <h2>Learned Monocular Depth Priors in Visual-Inertial Initialization</h2>
        <div class="title-line">
            <hr style = "width: 32%;">
        </div>
    </div>
   <div class ="authors">
  <h2 class="names">Yunwen Zhou*, Abhishek Kar, Eric Turner, Adarsh Kowdle, Chao X. Guo, Ryan C. DuToit, and Konstantine Tsotsos</h2>
      </div>

    <div class="logos-container">
        <div class="google-logo">
            <a href="https://https://arvr.google.com/ar/"><img src="thumbs/google.png" class="logos"></a>
        </div>

   <div class ="google-logo">
<h2 class="names">  <a href="https://eccv2022.ecva.net/">ECCV 2022</h2></a>
  </div>
	  
    </div>
    
    <div class="paper">
        <div class="grid-container">
            <div class="item1"><a href="total_relighting_paper.pdf"><img src="thumbs/tr_paper-1.jpg" class="paper-thumbnail"></a></div>
            <div class="item2"><a href="total_relighting_paper.pdf"><img src="thumbs/tr_paper-4.jpg" class="paper-thumbnail"></a></div>
            <div class="item3"><a href="total_relighting_paper.pdf"><img src="thumbs/tr_paper-9.jpg" class="paper-thumbnail"></a></div>
            <div class="item4"><a href="total_relighting_paper.pdf"><img src="thumbs/tr_paper-14.jpg" class="paper-thumbnail"></a></div>
        </div>
        <br>
        Click to view the paper.
    </div>

    <div class="abstract">
        <h2>Abstract</h2>
            <p style="text-align: justify;">
            <span class="first-letter">Visual-inertial odometry (VIO)</span> is the pose estimation backbone for most AR/VR and autonomous robotic systems today, in both academia and industry. However, these systems are highly sensitive to the initialization of key parameters such as sensor biases, gravity direction, and metric scale. In practical scenarios where high-parallax or variable acceleration assumptions are rarely met (e.g. hovering aerial robot, smartphone AR user not gesticulating with phone), classical visual-inertial initialization formulations often become ill-conditioned and/or fail to meaningfully converge. In this paper we target visual-inertial initialization specifically for these low-excitation scenarios critical to in-the-wild usage. We propose to circumvent the limitations of classical visual-inertial structure-from-motion (SfM) initialization by incorporating a new learning-based measurement as a higher-level input. We leverage learned monocular depth images (mono-depth) to constrain the relative depth of features, and upgrade the mono-depths to metric scale by jointly optimizing for their scales and shifts. Our experiments show a significant improvement in problem conditioning compared to a classical formulation for visual-inertial initialization, and demonstrate significant accuracy and robustness improvements relative to the state-of-the-art on public benchmarks, particularly under low-excitation scenarios. We further extend this improvement to implementation within an existing odometry system to illustrate the impact of our improved initialization method on resulting tracking trajectories.</p>
    </div>

	<div class="section list" >
		<h2>BibTex</h2>
		<div class="section bibtex">
		<pre>@inproceedings{verse:monodepth-vio-init-eccv2022,
		author = {Yunwen Zhou*, Abhishek Kar, Eric Turner, Adarsh Kowdle, Chao X. Guo, Ryan C. DuToit, and Konstantine Tsotsos},
		title = {Learned Monocular Depth Priors in Visual-Inertial Initialization},
        booktitle={The European Conference on Computer Vision},
		year = {2022},}
		</pre>
		</div>
	</div>
	
<!--     <div class="video">
            <h2 class="section-title">Video</h2>
            <div class="video-container" style="margin-top: 50px;">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/KeebkkaZhhI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div> 
            <br>
            SIGGRAPH 2021 Technical Video
    </div> -->


    <div class="footer">
        <hr style = "margin-bottom: 15px;">
        <div class="footer-googleai">
            <a href="https://https://arvr.google.com/ar" class="pagelink"><b>Google AR</b></a>
        </div>
        <div class="footer-google">
            <a href="https://google.com" class="pagelink"><b>Google</b></a>
        </div>
    </div>

</div>






</body>



